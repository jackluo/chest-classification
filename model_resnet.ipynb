{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jackluo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/jackluo/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1, 3\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Activation, Dropout\n",
    "from keras.layers import Conv2D, GlobalMaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "from densenet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % matplotlib inline\n",
    "# % config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "COLOR = 'rgb'\n",
    "IMG_SIZE = (299, 299)\n",
    "\n",
    "TRAIN_BATCH_SIZE = 48\n",
    "VALID_BATCH_SIZE = 48\n",
    "TEST_BATCH_SIZE = 128\n",
    "\n",
    "NB_EPOCHS = 100\n",
    "\n",
    "LR = 0.03\n",
    "NB_SNAPSHOTS = 5\n",
    "\n",
    "REDUCTION_FACTOR = 0.5\n",
    "REDUCTION_PATIENCE = 1\n",
    "MINIMUM_LR = 1e-6\n",
    "\n",
    "# WEIGHTS = None\n",
    "WEIGHTS = 'imagenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_idg = ImageDataGenerator(samplewise_center=True, \n",
    "                              samplewise_std_normalization=True, \n",
    "                              horizontal_flip = True, \n",
    "                              vertical_flip = False, \n",
    "                              height_shift_range=0.1, \n",
    "                              width_shift_range=0.05, \n",
    "                              shear_range=5,\n",
    "                              zoom_range=[0.94, 0.98],\n",
    "                              rotation_range=5,\n",
    "                              fill_mode = 'constant'\n",
    "                             )\n",
    "\n",
    "val_idg = ImageDataGenerator(samplewise_center=True, \n",
    "                             samplewise_std_normalization=True, \n",
    "                             zoom_range=[0.96, 0.96]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', \n",
    "              'Infiltration', 'Mass', 'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']\n",
    "\n",
    "# all_labels = ['Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax']\n",
    "\n",
    "data_dir = 'data'\n",
    "\n",
    "_dir = data_dir + '/split/' + 'df.csv'\n",
    "df = pd.read_csv(_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_counts(df, class_names):\n",
    "\n",
    "    total_count = df.shape[0]\n",
    "    labels = df[class_names].as_matrix()\n",
    "    positive_counts = np.sum(labels, axis=0)\n",
    "    class_positive_counts = dict(zip(class_names, positive_counts))\n",
    "    \n",
    "    return total_count, class_positive_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(total_counts, class_positive_counts, multiply):\n",
    "\n",
    "    def get_single_class_weight(pos_counts, total_counts):\n",
    "        denominator = (total_counts - pos_counts) * multiply + pos_counts\n",
    "        return {\n",
    "            0: pos_counts / denominator,\n",
    "            1: (denominator - pos_counts) / denominator,\n",
    "        }\n",
    "\n",
    "    class_names = list(class_positive_counts.keys())\n",
    "    label_counts = np.array(list(class_positive_counts.values()))\n",
    "    class_weights = []\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_weights.append(get_single_class_weight(label_counts[i], total_counts))\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_from_dataframe(img_data_gen, in_df, path_col, y_col, b, s):\n",
    "    \n",
    "    base_dir = os.path.dirname(in_df[path_col].values[0])\n",
    " \n",
    "    df_gen = img_data_gen.flow_from_directory(base_dir, \n",
    "                                              class_mode='sparse',\n",
    "                                              target_size=IMG_SIZE,\n",
    "                                              color_mode=COLOR,\n",
    "                                              batch_size=b,\n",
    "                                              shuffle=s)\n",
    "    \n",
    "    df_gen.filenames = in_df[path_col].values\n",
    "    df_gen.classes = np.stack(in_df[y_col].values)\n",
    "    df_gen.samples = in_df.shape[0]\n",
    "    df_gen.n = in_df.shape[0]\n",
    "    df_gen._set_index_array()\n",
    "    df_gen.directory = '' # since we have the full path\n",
    "    \n",
    "    print('Reinserting dataframe: {} images'.format(in_df.shape[0]))\n",
    "    \n",
    "    return df_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_anneal_schedule(epoch):\n",
    "    \n",
    "    cos_inner = np.pi * (epoch % (NB_EPOCHS // NB_SNAPSHOTS))\n",
    "    cos_inner /= NB_EPOCHS // NB_SNAPSHOTS\n",
    "    cos_outer = np.cos(cos_inner) + 1\n",
    "    \n",
    "    return float(LR / 2 * cos_outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['disease_vec'] = df.apply(lambda x: [x[all_labels].values], 1).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  78427\n",
      "Validation:  11747\n",
      "Test: 21946\n"
     ]
    }
   ],
   "source": [
    "# df_train, second_df = train_test_split(df, \n",
    "#                                        test_size=0.3, \n",
    "#                                        random_state=SEED)\n",
    "\n",
    "# df_valid, df_test = train_test_split(second_df, \n",
    "#                                      test_size=0.6666666666, \n",
    "#                                      random_state=SEED)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "unique_users = df['Patient ID'].unique()\n",
    "train_users, valid_users, test_users = np.split(\n",
    "    np.random.permutation(unique_users), [int(.7 * len(unique_users)), int(.8 * len(unique_users))]\n",
    ")\n",
    "\n",
    "df_train = df[df['Patient ID'].isin(train_users)]\n",
    "df_valid = df[df['Patient ID'].isin(valid_users)]\n",
    "df_test = df[df['Patient ID'].isin(test_users)]\n",
    "\n",
    "print('Train: ', df_train.shape[0])\n",
    "print('Validation: ', df_valid.shape[0])\n",
    "print('Test:', df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n",
      "Reinserting dataframe: 78427 images\n",
      "Found 0 images belonging to 0 classes.\n",
      "Reinserting dataframe: 11747 images\n",
      "Found 0 images belonging to 0 classes.\n",
      "Reinserting dataframe: 21946 images\n"
     ]
    }
   ],
   "source": [
    "train_gen = flow_from_dataframe(core_idg, \n",
    "                                df_train, \n",
    "                                path_col='path',\n",
    "                                y_col='disease_vec',\n",
    "                                b=TRAIN_BATCH_SIZE, \n",
    "                                s=True)\n",
    "\n",
    "valid_gen = flow_from_dataframe(val_idg, \n",
    "                                df_valid, \n",
    "                                path_col='path',\n",
    "                                y_col='disease_vec',\n",
    "                                b=VALID_BATCH_SIZE, \n",
    "                                s=False)\n",
    "\n",
    "test_gen = flow_from_dataframe(val_idg, \n",
    "                               df_test, \n",
    "                               path_col='path',\n",
    "                               y_col='disease_vec',\n",
    "                               b=TEST_BATCH_SIZE, \n",
    "                               s=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = next(train_gen)\n",
    "X_valid, y_valid = next(valid_gen)\n",
    "\n",
    "# fig, m_axs = plt.subplots(4, 4, figsize = (16, 16))\n",
    "# for (c_x, c_y, c_ax) in zip(X_train, y_train, m_axs.flatten()):\n",
    "#      c_ax.imshow(c_x[:,:,0], cmap = 'bone', vmin = -1.5, vmax = 1.5)\n",
    "#      c_ax.set_title(', '.join([n_class for n_class, n_score in zip(all_labels, c_y) \n",
    "#                                if n_score>0.5]))\n",
    "#      c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 0.1015594119372155, 1: 0.8984405880627845},\n",
       " {0: 0.023601565787292642, 1: 0.9763984342127073},\n",
       " {0: 0.04210284723373328, 1: 0.9578971527662667},\n",
       " {0: 0.01981460466421003, 1: 0.98018539533579},\n",
       " {0: 0.11938490570849325, 1: 0.8806150942915068},\n",
       " {0: 0.02140844352072628, 1: 0.9785915564792738},\n",
       " {0: 0.015211598046591097, 1: 0.9847884019534089},\n",
       " {0: 0.002078365868897191, 1: 0.9979216341311028},\n",
       " {0: 0.17667384956711338, 1: 0.8233261504328866},\n",
       " {0: 0.05207390312009894, 1: 0.947926096879901},\n",
       " {0: 0.0571104339066903, 1: 0.9428895660933097},\n",
       " {0: 0.030882221683858874, 1: 0.9691177783161411},\n",
       " {0: 0.012495696635087407, 1: 0.9875043033649126},\n",
       " {0: 0.047088375176916115, 1: 0.9529116248230839}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts, train_pos_counts = get_sample_counts(df_train, all_labels)\n",
    "class_weights = get_class_weights(train_counts, train_pos_counts, multiply=1)\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "sgd = optimizers.SGD(decay=1e-5, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = DenseNet([6, 12, 48, 32], growth_rate=32,\n",
    "#              include_top=False, weights=WEIGHTS, \n",
    "#              input_tensor=None, input_shape=X_train.shape[1:], \n",
    "#              pooling=None, classes=len(all_labels))\n",
    "\n",
    "base_model = keras.applications.inception_resnet_v2.InceptionResNetV2(\n",
    "    include_top=False, weights=WEIGHTS, \n",
    "    input_tensor=None, input_shape=X_train.shape[1:], \n",
    "    pooling=None, classes=len(all_labels)\n",
    ")\n",
    "\n",
    "# x = GlobalMaxPooling2D()(base_model.output)\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "predictions = Dense(len(all_labels), activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "parallel_model = multi_gpu_model(model, gpus=3)\n",
    "\n",
    "# parallel_model.load_weights('weights.h5')\n",
    "parallel_model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy', 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('weights_{epoch:02d}.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=False, mode='min', save_weights_only=True)\n",
    "\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=REDUCTION_FACTOR, patience=REDUCTION_PATIENCE,\n",
    "                              verbose=1, mode=\"min\", min_lr=MINIMUM_LR)\n",
    "\n",
    "lr_schedule = LearningRateScheduler(schedule=cosine_anneal_schedule, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, lr_schedule]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.03.\n",
      " 645/1633 [==========>...................] - ETA: 25:02 - loss: 0.1734 - acc: 0.9466 - categorical_accuracy: 0.1813"
     ]
    }
   ],
   "source": [
    "parallel_model.fit_generator(train_gen, \n",
    "                             initial_epoch=0,\n",
    "                             steps_per_epoch=int(df_train.shape[0]/TRAIN_BATCH_SIZE),\n",
    "                             epochs=NB_EPOCHS,  \n",
    "                             validation_data=valid_gen, \n",
    "                             validation_steps=int(df_valid.shape[0]/VALID_BATCH_SIZE),\n",
    "                             class_weight=class_weights,\n",
    "                             callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel_model.load_weights('weights.h5')\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df = df_test['disease_vec'].values\n",
    "\n",
    "y_test = []\n",
    "\n",
    "for i in y_test_df:\n",
    "    y_test.append(i)\n",
    "    \n",
    "y_test = np.array(y_test)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = parallel_model.predict_generator(test_gen, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.titleweight'] = 'bold'\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(9, 9))\n",
    "\n",
    "for (idx, c_label) in enumerate(all_labels):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test[:, idx].astype(int), y_pred[:, idx])\n",
    "    ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "    \n",
    "ax.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n",
    "ax.set_title('ROC Curve')\n",
    "ax.set_xlabel('1 - Specificity')\n",
    "ax.set_ylabel('Sensitivity')\n",
    "ax.set_xlim([-0.01, 1.01])\n",
    "ax.set_ylim([-0.01, 1.01])\n",
    "\n",
    "fig.savefig('roc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
